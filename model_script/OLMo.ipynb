{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0,1\"\n",
    "# os.environ['PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION'] = 'upb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TRANSFORMERS_CACHE'] = './models'\n",
    "print(f\"Current cache directory: {os.environ['TRANSFORMERS_CACHE']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoModel\n",
    "from transformers import pipeline\n",
    "\n",
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"OLMo-7B-0724-Instruct-hf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a056eab933c64643955e3651709b3f5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/637 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "547b5b6882934742a8cb4a36d3102e0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/18.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8511aa94fee4a54b40782e040d2a6dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70772256f28a4033b1ce5247f392d784",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00003.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86bd80b73fdf4b77a7586628b9c8e87c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00003.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "810991b6937c4edd9c9e848b79fb8a7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00003.safetensors:   0%|          | 0.00/3.83G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b26181ba10054b0c832e8c2dd72a9c14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88c381d6059b499883caefde13e64ed7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/115 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d791677be5414800897320f6fe8b4051",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/5.73k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a57efbf451d144ea916fc08f51b12b13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.12M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6be1339e778644b0b9e0a6c0af56c7e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/293 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pipe = pipeline(\"text-generation\", model=\"allenai/{}\".format(model), max_length=2048, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_unicode_to_plain_text(unicode_str):\n",
    "    if isinstance(unicode_str, str) and '\\\\u' in unicode_str:\n",
    "        plain_text = unicode_str.encode().decode('unicode_escape')\n",
    "        return plain_text\n",
    "    else:\n",
    "        return unicode_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"Give me a sense of your preference regarding the following question or statement. Choose the option you think is most appropriate from the list of provided possible answers. \\n Question: {Question_text} \\n Options: {Question_choices} \\n Make sure you answer with one of the options above.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "para_dict = [{\n",
    "    \"role\": \"user\",\n",
    "    \"content\": \"\"\n",
    "    }]\n",
    "\n",
    "para_dict2 = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Some people have a political issue that they care about more than most other issues. They might think about the issue a lot. They might pay particular attention to news about that issue, even when it's not making national news. They might focus on what political candidates say about that issue, and decide who to vote for on the basis of that issue. Or they might just care about the issue a lot. Is there an issue like that for you?\"\n",
    "    },\n",
    "    {\"role\": \"assistant\",\n",
    "        \"content\": \"Yes\"}, \n",
    "    {\"role\": \"user\",\n",
    "        \"content\":  \"\"}\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = \"./question_json_ver3\"\n",
    "script_list = os.listdir(root)\n",
    "script_list.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on questions_json_1.json ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "45it [01:29,  1.99s/it]\n",
      " 10%|█         | 1/10 [01:29<13:26, 89.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on questions_json_10.json ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "45it [01:25,  1.90s/it]\n",
      " 20%|██        | 2/10 [02:54<11:36, 87.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on questions_json_2.json ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "45it [01:27,  1.95s/it]\n",
      " 30%|███       | 3/10 [04:22<10:11, 87.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on questions_json_3.json ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "45it [01:28,  1.97s/it]\n",
      " 40%|████      | 4/10 [05:51<08:47, 87.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on questions_json_4.json ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "45it [01:29,  1.98s/it]\n",
      " 50%|█████     | 5/10 [07:20<07:21, 88.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on questions_json_5.json ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "45it [01:22,  1.83s/it]\n",
      " 60%|██████    | 6/10 [08:43<05:45, 86.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on questions_json_6.json ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "45it [01:25,  1.90s/it]\n",
      " 70%|███████   | 7/10 [10:08<04:18, 86.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on questions_json_7.json ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "45it [01:30,  2.01s/it]\n",
      " 80%|████████  | 8/10 [11:38<02:54, 87.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on questions_json_8.json ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "45it [01:22,  1.83s/it]\n",
      " 90%|█████████ | 9/10 [13:01<01:25, 85.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on questions_json_9.json ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "45it [01:29,  1.98s/it]\n",
      "100%|██████████| 10/10 [14:30<00:00, 87.01s/it]\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(model): os.mkdir(model)\n",
    "\n",
    "for script in tqdm(script_list):\n",
    "    if not os.path.exists(os.path.join(model, script)):\n",
    "# if 1:\n",
    "#         script = script_list[0]\n",
    "        print(\"working on {} ...\".format(script))\n",
    "\n",
    "        with open(os.path.join(root, script), 'r') as f:\n",
    "            data = json.load(f)\n",
    "        for i, question in tqdm(enumerate(data['questions'])):\n",
    "            execute = 0\n",
    "            if \"Answer_text\" not in question.keys():\n",
    "                q, c = question[\"Question_text\"], question[\"Question_choices\"]\n",
    "                if len(c) > 0:\n",
    "                    execute = 1\n",
    "                    template_tmp = template.format(**({\"Question_text\":q, \"Question_choices\":c}))\n",
    "\n",
    "                    para_tmp = para_dict.copy()\n",
    "                    para_tmp[-1][\"content\"] = template_tmp\n",
    "                elif len(c) == 0 and (\"yes\" in data['questions'][i-1][\"Answer_text\"].lower()):\n",
    "                    execute = 1\n",
    "                    template_tmp = q\n",
    "\n",
    "                    para_tmp = para_dict2.copy()\n",
    "                    para_tmp[-1][\"content\"] = template_tmp\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "                if execute:\n",
    "                    try:\n",
    "                            response = pipe(para_tmp)\n",
    "                            output = response[-1]['generated_text'][-1][\"content\"] \n",
    "                    except:\n",
    "                            response = \"#ERROR\"\n",
    "                    data['questions'][i][\"Answer_text\"] = convert_unicode_to_plain_text(output)\n",
    "                    \n",
    "                with open(os.path.join(model, script), 'w', encoding='utf-8') as f:\n",
    "                    json.dump(data, f, indent=4)\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Political",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
