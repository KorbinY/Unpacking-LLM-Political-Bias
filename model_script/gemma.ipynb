{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"2,3,4,5\"\n",
    "# os.environ['PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION'] = 'upb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['HF_HOME'] = './models'\n",
    "print(f\"Current cache directory: {os.environ['HF_HOME']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jailbreak import jailbreak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoModel\n",
    "from transformers import pipeline\n",
    "\n",
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = \"gemma-2-2b-it\"\n",
    "# model = \"gemma-2-9b-it\"\n",
    "model = \"gemma-2-27b-it\"\n",
    "\n",
    "pre_prompt = jailbreak.replace(\"{LLM_name}\", \"GEMMA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e8da2d499634385b4dfba5aff349e0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pipe = pipeline(\"text-generation\", model=\"google/{}\".format(model), max_new_tokens=2048, device_map=\"auto\", token=\"\")\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"google/{}\".format(model), token=\"\")\n",
    "# llm = AutoModelForCausalLM.from_pretrained(\"google/{}\".format(model), device_map=\"auto\", torch_dtype=torch.bfloat16, token=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_unicode_to_plain_text(unicode_str):\n",
    "    if isinstance(unicode_str, str) and '\\\\u' in unicode_str:\n",
    "        plain_text = unicode_str.encode().decode('unicode_escape')\n",
    "        return plain_text\n",
    "    else:\n",
    "        return unicode_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"Give me a sense of your preference regarding the following question or statement. Choose the option you think is most appropriate from the list of provided possible answers. \\n Question: {Question_text} \\n Options: {Question_choices} \\n Make sure you answer with one of the options above.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "template2 = \"Give me a sense of your preference regarding the following question or statement. Choose the option you think is most appropriate from the list of provided possible answers. \\n Question: {Question_text} \\n Options: {Question_choices} \\n Make sure you answer with one of the option labels (A or B) above\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "para_dict = [{\n",
    "    \"role\": \"user\",\n",
    "    \"content\": \"\"\n",
    "    }]\n",
    "\n",
    "para_dict2 = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"In the United States, Some people have a political issue that they care about more than most other issues. They might think about the issue a lot. They might pay particular attention to news about that issue, even when it's not making national news. They might focus on what political candidates say about that issue, and decide who to vote for on the basis of that issue. Or they might just care about the issue a lot. Is there an issue like that for you?\"\n",
    "    },\n",
    "    {\"role\": \"assistant\",\n",
    "        \"content\": \"Yes\"}, \n",
    "    {\"role\": \"user\",\n",
    "        \"content\":  \"\"}\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = \"./question_json_ver3\"\n",
    "script_list = os.listdir(root)\n",
    "script_list.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(model): os.mkdir(model)\n",
    "\n",
    "for script in tqdm(script_list):\n",
    "    # if not os.path.exists(os.path.join(model, script)):\n",
    "# if 1:\n",
    "#         script = script_list[0]\n",
    "        print(\"working on {} ...\".format(script))\n",
    "\n",
    "        if not os.path.exists(os.path.join(model, script)):\n",
    "            with open(os.path.join(root, script), 'r') as f:\n",
    "                data = json.load(f)\n",
    "        else:\n",
    "            with open(os.path.join(model, script), 'r') as f:\n",
    "                data = json.load(f)\n",
    "\n",
    "        for i, question in tqdm(enumerate(data['questions'])):\n",
    "            execute = 0\n",
    "            if question.get(\"Answer_text\", \"#ERROR\") == \"#ERROR\":\n",
    "                q, c = question[\"Question_text\"], question[\"Question_choices\"]\n",
    "                if len(c) > 0:\n",
    "                    execute = 1\n",
    "                    if not \"Misinformation\" in question[\"Section\"]:\n",
    "                        template_tmp = template.format(**({\"Question_text\":q, \"Question_choices\":c}))\n",
    "                    else:\n",
    "                        template_tmp = template2.format(**({\"Question_text\":q, \"Question_choices\":c}))\n",
    "\n",
    "                    para_tmp = para_dict.copy()\n",
    "                    para_tmp[-1][\"content\"] = pre_prompt + template_tmp\n",
    "                elif len(c) == 0 and (\"yes\" in data['questions'][i-1][\"Answer_text\"].lower()):\n",
    "                    execute = 1\n",
    "                    template_tmp = q\n",
    "\n",
    "                    para_tmp = para_dict2.copy()\n",
    "                    para_tmp[-1][\"content\"] = pre_prompt + template_tmp\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "                if execute:\n",
    "                    # try:\n",
    "                            output = pipe(para_tmp)\n",
    "                            output = output[-1]['generated_text'][-1]['content']\n",
    "                    # except:\n",
    "                    #         response = \"#ERROR\"\n",
    "                            data['questions'][i][\"Answer_text\"] = convert_unicode_to_plain_text(output)\n",
    "                            data['questions'][i][\"LLM_name\"] = model\n",
    "                    \n",
    "                with open(os.path.join(model, script), 'w', encoding='utf-8') as f:\n",
    "                    json.dump(data, f, indent=4)\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Political",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
